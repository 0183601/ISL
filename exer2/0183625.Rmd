---
title: "0183625"
author: "ZXH0183625"
date: "2021/5/15"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Q 10
```{r}
library(ISLR)
library(caret)
library(MASS)
library(class)
```
## a)
```{r}
summary(Weekly)
pairs(Weekly)
```
There is a positive correlation between Year and Volume.

## b)
```{r}
fit.logis = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(fit.logis)
```
Lag2 has statistical significance and its p-value is 0.0296.

## c)
```{r}
prob.logis = predict(fit.logis, type ="response")  #预测响应变量为1的概率
pred.logis = rep("Down", length(prob.logis)) 
pred.logis[prob.logis > 0.5] = "Up"  #根据预测概率确定0-1变量预测结果
# table(pred.logis, Weekly$Direction)
# mean(pred.logis == Weekly$Direction)
con.logis = confusionMatrix(data = as.factor(pred.logis), Weekly$Direction)
con.logis
```
The accuracy is 56.11%. 
The rate which the prediction is right with the down direction is 11.157%.
The rate which the prediction is right with the up direction is 92.066%.


## d)
```{r}
train = Weekly[Weekly$Year < 2009, ]
test = Weekly[Weekly$Year > 2008, ]
fit.logis.t = glm(Direction ~ Lag2, data = train, family = binomial)
prob.logis.t = predict(fit.logis.t, test, type = "response")
pred.logis.t = rep("Down", length(prob.logis.t))
pred.logis.t[prob.logis.t > 0.5] = "Up"
con.logis.t = confusionMatrix(data = as.factor(pred.logis.t), test$Direction)
con.logis.t
```
The accuracy is 62.5%.

## e)
```{r}
fit.lda = lda(Direction ~ Lag2, data = train)
pred.lda = predict(fit.lda, test)
con.lda = confusionMatrix(data = as.factor(pred.lda$class), test$Direction)
con.lda
```

## f)
```{r}
fit.qda = qda(Direction ~ Lag2, data = train)
pred.qda = predict(fit.qda, test)
con.qda = confusionMatrix(data = as.factor(pred.qda$class), test$Direction)
con.qda
```

## g)
```{r}
set.seed(20210515)
pred.knn = knn(as.matrix(train$Lag2), as.matrix(test$Lag2), train$Direction, k = 1)
con.knn = confusionMatrix(data = as.factor(pred.knn), test$Direction)
con.knn
```

## h）
Logistic regression and LDA methods are better. Because they provide the similar test error rates.

## i)
```{r}
# Logistic regression
fit.logis2 = glm(Direction ~ Lag2:Lag1, data = train, family = binomial)
prob.logis2 = predict(fit.logis2, test, type = "response")
pred.logis2 = rep("Down", length(prob.logis2))
pred.logis2[prob.logis2 > 0.5] = "Up"
con.logis2 = confusionMatrix(data = as.factor(pred.logis2), test$Direction)
con.logis2

```
```{r}
# LDA
fit.lda2 = lda(Direction ~ Lag2:Lag1, data = train)
pred.lda2 = predict(fit.lda2, test)
con.lda2 = confusionMatrix(data = as.factor(pred.lda2$class), test$Direction)
con.lda2
```
```{r}
# QDA
fit.qda2 = qda(Direction ~ Lag2:Lag1, data = train)
pred.qda2 = predict(fit.qda2, test)
con.qda2 = confusionMatrix(data = as.factor(pred.qda2$class), test$Direction)
con.qda2
```
```{r}
# KNN
set.seed(20210515)
pred.knn2 = knn(as.matrix(train$Lag2), as.matrix(test$Lag2), train$Direction, k = 20)
con.knn2 = confusionMatrix(data = as.factor(pred.knn2), test$Direction)
con.knn2
```



# Q11
```{r}
library(ISLR)
library(MASS)
library(class)
```
## a)
```{r}
attach(Auto)
mpg01 = rep(0, length(mpg))
mpg01[mpg > median(mpg)] = 1
Auto = data.frame(Auto, mpg01)
head(Auto)
```

## b)
```{r}
pairs(Auto)
boxplot(mpg ~ cylinders, data = Auto)
plot(mpg, weight)
plot(mpg, displacement)
plot(mpg, horsepower)
```
Cylinders, weight, displacement, horsepower have an impact on predicting mpg.

## c)
```{r}
train.Auto = Auto[Auto$year < 79, ]
test.Auto = Auto[Auto$year > 78, ]
train.mpg01 = mpg01[Auto$year < 79]
test.mpg01 = mpg01[Auto$year > 78]
```

## d)
```{r}
auto.fit.lda = lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = train.Auto)
auto.pred.lda = predict(auto.fit.lda, test.Auto)
mean(auto.pred.lda$class != test.mpg01)
```
The test error is 14.91228%.

## e)
```{r}
auto.fit.qda = qda(mpg01 ~ cylinders + weight + displacement + horsepower, data = train.Auto)
auto.pred.qda = predict(auto.fit.qda, test.Auto)
mean(auto.pred.qda$class != test.mpg01)
```
The test error is 17.54386%.

## f)
```{r}
auto.fit.logis = glm(mpg01 ~ cylinders + weight + displacement + horsepower, 
                     data = train.Auto, family = binomial)
auto.prob.logis = predict(auto.fit.logis, test.Auto, type = "response")
auto.pred.logis = rep("0", length(auto.prob.logis))
auto.pred.logis[auto.prob.logis > 0.5] = "1"
mean(auto.pred.logis != test.mpg01)
```
The test error is 19.29825%.

## g)
```{r}
set.seed(20210515)
traink = cbind(cylinders, weight, displacement, horsepower)[Auto$year < 79, ]
testk = cbind(cylinders, weight, displacement, horsepower)[Auto$year > 78, ]
# K = 1
auto.pred.knn1 = knn(traink, testk, train.mpg01, k = 1)
mean(auto.pred.knn1 != test.mpg01)
# K = 10
auto.pred.knn2 = knn(traink, testk, train.mpg01, k = 10)
mean(auto.pred.knn2 != test.mpg01)
# K = 20
auto.pred.knn3 = knn(traink, testk, train.mpg01, k = 20)
mean(auto.pred.knn3 != test.mpg01)
```
The test error with K of 1 is 22.80702%.
The test error with K of 10 is 19.29825%.
The test error with K of 20 is 16.66667%.
The result with K of 20 perform the best.