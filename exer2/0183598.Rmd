---
title: "0183598"
output: html_document
---

# 10

## a
```{r}
library(ISLR)
summary(Weekly)
pairs(Weekly)
```
我们可以看出大部分数据集中在一个小区间中，还有一些异常值
Lags和当日的投资回报率的值很接近，在预测的走向中，跌的天数比涨的更多。


## b
```{r}
attach(Weekly)
glm.fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly,  family = binomial)
summary(glm.fit)
 
```
Pr(>|z|)小的值越有显著性，Lag2因此时是显著的统计变量。

## c
```{r}
glm.probs = predict(glm.fit, type = "response")
glm.pred = rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] = "Up"
table(glm.pred, Direction)
(54+557)/(54+48+430+557)

```
准确率是56.1%
FP = 430/(54 + 430) = 90.7%
FN = 48/(48 + 557) = 7.9%
所以这个错误应该是第I类错误。


## d
```{r}
train = (Year < 2009)
Weekly.0910 = Weekly[!train, ]
glm.fit = glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
glm.probs = predict(glm.fit, Weekly.0910, type = "response")
glm.pred = rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] = "Up"
Direction.0910 = Direction[!train]
table(glm.pred, Direction.0910)
mean(glm.pred == Direction.0910)
```
总体预测率是62.5%

## e
```{r}
library(MASS)
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.pred = predict(lda.fit, Weekly.0910)
table(lda.pred$class, Direction.0910)
mean(lda.pred$class == Direction.0910)
```
应用LDA的准确率仍是62.5%。

## f
```{r}
qda.fit = qda(Direction ~ Lag2, data = Weekly, subset = train)
qda.class = predict(qda.fit, Weekly.0910)$class
table(qda.class, Direction.0910)
mean(qda.class == Direction.0910)
```
应用QDA的准确率是58.6%。

## g
```{r}
library(class)
train.X = as.matrix(Lag2[train])
test.X = as.matrix(Lag2[!train])
train.Direction = Direction[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.0910)
mean(knn.pred == Direction.0910)
```
应用K=1的KNN的准确率是50%。

## h
逻辑斯蒂回归和线性判别分析的方法好于二次判别分析和k最近邻法。

## i
```{r}
# Logistic regression with Lag2+Lag1
glm.fit = glm(Direction ~ Lag2+Lag1, data = Weekly, family = binomial, subset = train)
glm.probs = predict(glm.fit, Weekly.0910, type = "response")
glm.pred = rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] = "Up"
Direction.0910 = Direction[!train]
table(glm.pred, Direction.0910)
mean(glm.pred == Direction.0910)

# LDA with Lag2：Lag4
lda.fit = lda(Direction ~ Lag2:Lag4, data = Weekly, subset = train)
lda.pred = predict(lda.fit, Weekly.0910)
table(lda.pred$class,Direction.0910)
mean(lda.pred$class == Direction.0910)

# QDA with sqrt(abs(Lag1))
qda.fit = qda(Direction ~ Lag2 + sqrt(abs(Lag1)), data = Weekly, subset = train)
qda.class = predict(qda.fit, Weekly.0910)$class
table(qda.class, Direction.0910)
mean(qda.class == Direction.0910)

# KNN k =1
knn.pred = knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.0910)
mean(knn.pred == Direction.0910)

# KNN k =10
knn.pred = knn(train.X, test.X, train.Direction, k = 10)
table(knn.pred, Direction.0910)
mean(knn.pred == Direction.0910)


# KNN k = 20
knn.pred = knn(train.X, test.X, train.Direction, k = 20)
table(knn.pred, Direction.0910)
mean(knn.pred == Direction.0910)

```

通过上述结果可以看出，logistics回归效果最好。变量是Lag2+Lag1，相关混淆矩阵是Direction.0910
glm.pred Down Up
    Down    7  8
    Up     36 53
[1] 0.5769231

# 11

## a
```{r}
library(ISLR)
summary(Auto)
attach(Auto)
mpg01 = rep(0, length(mpg))
mpg01[mpg > median(mpg)] = 1
Auto = data.frame(Auto, mpg01)
```
## b
```{r}
plot(cylinders,mpg01)
plot(displacement,mpg01)
plot(acceleration,mpg01)
plot(horsepower,mpg01)
plot(weight,mpg01)
plot(year,mpg01)
plot(origin,mpg01)

```
```{r}
mpg01=as.factor(mpg01)
plot(mpg01,cylinders)
plot(mpg01,year)
plot(mpg01,origin)
plot(mpg01,acceleration)
plot(mpg01,displacement)
plot(mpg01,horsepower)
plot(mpg01,weight)
```

从散点图来看，mpg01和association ，weight, horsepower, acceleration,displacement
有关系。
从箱线图来看，cylinders是更相关的。


## c
```{r}
train = (year%%2 == 0) 
Auto.train = Auto[train, ]
Auto.test = Auto[!train, ]
mpg01.test = mpg01[!train]
```


## d
```{r}
library(MASS)
lda.fit = lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, subset = train)
lda.pred = predict(lda.fit, Auto.test)
mean(lda.pred$class != mpg01.test)
```
测试误差是10.6%

## e
```{r}
qda.fit = qda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, subset = train)
qda.pred = predict(qda.fit, Auto.test)
mean(qda.pred$class != mpg01.test)
```
测试误差是13.5%


## f
```{r}
glm.fit = glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, family = binomial, subset = train)
glm.probs = predict(glm.fit, Auto.test, type = "response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] = 1
mean(glm.pred != mpg01.test)
```
测试误差是21.9%。

## g
```{r}
library(class)
train.X = cbind(cylinders, weight, displacement, horsepower)[train, ]
test = !train
test.X = cbind(cylinders, weight, displacement, horsepower)[test, ]
train.mpg01 = mpg01[train]
set.seed(1)
# KNN(k=1)
knn.pred = knn(train.X, test.X, train.mpg01, k = 1)
mean(knn.pred != mpg01.test)

# KNN(K=5)
knn.pred = knn(train.X, test.X, train.mpg01, k = 5)
mean(knn.pred != mpg01.test)

# KNN(k=10)
knn.pred = knn(train.X, test.X, train.mpg01, k = 10)
mean(knn.pred != mpg01.test)

# KNN(k=100)
knn.pred = knn(train.X, test.X, train.mpg01, k = 100)
mean(knn.pred != mpg01.test)
```
k=1时测试误差是18.5%
k=5时测试误差时15.2%
k=10和k=100时测试误差时16.9%
K取5时KNN在数据集上效果最好。